{"componentChunkName":"component---src-templates-blog-post-js","path":"/llama2-local-devbox/","result":{"data":{"site":{"siteMetadata":{"title":"Canhua's Blog"}},"markdownRemark":{"id":"0c18af70-7394-53a5-81bb-a977c884e412","excerpt":"learn llama2 I tried multiple openai API implementation(eg: gpt4all) for llama2, [llama-cpp-python] provides the best compatiblity with openai API. llama-cppâ€¦","html":"<h1>learn llama2</h1>\n<p>I tried multiple openai API implementation(eg: <a href=\"https://github.com/nomic-ai/gpt4all\">gpt4all</a>) for llama2, [llama-cpp-python] provides the best compatiblity with openai API.</p>\n<p>llama-cpp-python offers a web server which aims to act as a drop-in replacement for the OpenAI API. This allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).</p>\n<h2>Setup LLaMa2 + openai API endpoint on Ubuntu 22.04 without GPU</h2>\n<p>You are experience slow response with 8G memory. You can get good performance with 16G memory</p>\n<h3>Create VM on Azure (optional)</h3>\n<p>Create VM with Ubuntu Minimal 22.04 LTS, Standard D4s v3 (4 vcpus, 16 GiB memory) prefered.</p>\n<h3>Install llama-cpp-python[server]</h3>\n<pre><code class=\"language-sh\">sudo apt-get install pip\r\npip install llama-cpp-python[server]\n</code></pre>\n<h3>Download the model</h3>\n<p>You may choose the model from <a href=\"https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\">https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF</a>.</p>\n<pre><code class=\"language-sh\">wget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n</code></pre>\n<h3>Launch llama-cpp-python server</h3>\n<pre><code>export HOST=0.0.0.0\r\nexport PORT=8080\r\npython3 -m llama_cpp.server --model `pwd`/llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf &#x26;\n</code></pre>\n<h3>Verify the endpoint.</h3>\n<p>llama-cpp-python provides implementation of <a href=\"https://platform.openai.com/docs/guides/gpt\">openai</a> interface. Just replace <code>https://api.openai.com</code> with <code>http://{yourip}:8080</code>, then you are good to go.</p>\n<p>For example, the completions API:</p>\n<pre><code>https://api.openai.com/v1/completions\n</code></pre>\n<pre><code>http://{yourip}:8080/v1/completions\n</code></pre>","frontmatter":{"title":"LLaMa2 local devbox setup and openai API server","date":"October 02, 2023","description":"Setup local LLama2 devbox and launch openai API server"}},"previous":{"fields":{"slug":"/ruby1/"},"frontmatter":{"title":"self and class<< in ruby"}},"next":null},"pageContext":{"id":"0c18af70-7394-53a5-81bb-a977c884e412","previousPostId":"0d92f0fd-a602-5db5-ba0a-e2ad3d95ed51","nextPostId":null}},"staticQueryHashes":["2841359383","3257411868"],"slicesMap":{}}